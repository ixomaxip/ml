{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ors/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import numpy as  np\n",
    "import os, random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "model = r\"SBW-vectors-300-min5.txt\"\n",
    "train=r\"./data/haha_2019_train_preprocessed_lemmatized.csv\"\n",
    "test=r\"./data/haha_2019_test_preprocessed_lemmatized.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) \n",
    "    text = re.sub('[¡¿.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w)>=3]\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    text=' '.join(words)\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(i) for i in tokens]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntexts=df[1].tolist()\\nscores=df[9].tolist()\\n\\ncategories=[1 if str(s)!='nan' else 0 for s in scores]\\n\\ndf = pd.read_csv(test, sep=',', header=None)\\ntexts_test=df[1].tolist()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read csv\n",
    "\n",
    "values= pd.read_csv(train, sep=',', header=None, encoding = 'utf-8-sig').values\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(values)\n",
    "#df=pd.DataFrame(values)\n",
    "\n",
    "m = len(values)\n",
    "\n",
    "train_length = int(0.9 * m)\n",
    "train_data, test_data = values[:train_length], values[train_length:]\n",
    "\n",
    "df=pd.DataFrame(train_data)\n",
    "\n",
    "texts_train=df[1].tolist()\n",
    "scores_train=df[9].tolist()\n",
    "categories_train=[1 if str(s)!='nan' else 0 for s in scores_train]\n",
    "\n",
    "df=pd.DataFrame(test_data)\n",
    "\n",
    "texts_test=df[1].tolist()\n",
    "scores_test=df[9].tolist()\n",
    "categories_test=[1 if str(s)!='nan' else 0 for s in scores_test]\n",
    "'''\n",
    "texts=df[1].tolist()\n",
    "scores=df[9].tolist()\n",
    "\n",
    "categories=[1 if str(s)!='nan' else 0 for s in scores]\n",
    "\n",
    "df = pd.read_csv(test, sep=',', header=None)\n",
    "texts_test=df[1].tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 16734\n"
     ]
    }
   ],
   "source": [
    "words=set()#set of all words\n",
    "for text in texts_train:\n",
    "    words_text=clean_text(text)\n",
    "    words.update(words_text)\n",
    "print(\"number of words: {0}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100000\n",
      "iteration 200000\n",
      "iteration 300000\n",
      "iteration 400000\n",
      "iteration 500000\n",
      "iteration 600000\n",
      "iteration 700000\n",
      "iteration 800000\n",
      "iteration 900000\n",
      "iteration 1000000\n",
      "size of dictionary: 12357\n"
     ]
    }
   ],
   "source": [
    "embdict=dict()#dictionary for words and emb\n",
    "index=0\n",
    "\n",
    "with open(model,'r',encoding = 'utf-8-sig')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word=str(f.readline()).replace('b','').replace('\\'','').replace('\\\\n','').lower().split()\n",
    "        w = stemmer.stem(word[0])\n",
    "        if w in words:\n",
    "            word.remove(word[0])\n",
    "            emb = [float(x) for x in word]\n",
    "            embdict[str(w)]=emb\n",
    "        index+=1\n",
    "        if index%100000==0:\n",
    "            print(\"iteration \"+str(index))\n",
    "\n",
    "print(\"size of dictionary: {0}\".format(len(embdict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "предобработка текста\n",
      "2400\n",
      "2400\n",
      "20482\n",
      "2256\n",
      "Максимальное количество слов в самом длинном тексте: 3257 слов\n",
      "Всего уникальных слов в словаре: 23586\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "prep_texts_train =[]\n",
    "prep_texts_test =[]\n",
    "\n",
    "print(\"предобработка текста\")\n",
    "            \n",
    "for t in texts_test:            \n",
    "    prep_texts_train.append(' '.join(clean_text(t)))\n",
    "for t in texts_test:\n",
    "    prep_texts_test.append(' '.join(clean_text(t)))\n",
    "    \n",
    "print(len(prep_texts_train))\n",
    "print(len(prep_texts_test))\n",
    "\n",
    "prep_texts_train1 =[]\n",
    "prep_texts_test1 =[]  \n",
    "cats_train=[]\n",
    "cats_test=[]\n",
    "i=0\n",
    "\n",
    "for t in texts_train:\n",
    "    boo=False\n",
    "    temp=t.split()\n",
    "    for t1 in temp:\n",
    "        if t1.lower() in embdict:\n",
    "            boo=True\n",
    "            break\n",
    "    if boo:\n",
    "        prep_texts_train1.append(t)\n",
    "        cats_train.append(categories_train[i])\n",
    "    i+=1\n",
    "\n",
    "print(len(prep_texts_train1))\n",
    "\n",
    "i=0\n",
    "\n",
    "for t in texts_test:\n",
    "    boo=False\n",
    "    temp=t.split()\n",
    "    for t1 in temp:\n",
    "        if t1.lower() in embdict:\n",
    "            boo=True\n",
    "            break\n",
    "    if boo:\n",
    "        prep_texts_test1.append(t)\n",
    "        cats_test.append(categories_test[i])\n",
    "    i+=1\n",
    "    \n",
    "print(len(prep_texts_test1))\n",
    "\n",
    "descriptions = prep_texts_train1\n",
    "    \n",
    "x_train = prep_texts_train1\n",
    "y_train = cats_train\n",
    "    \n",
    "x_test = prep_texts_test1\n",
    "y_test = cats_test\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "max_words = 0\n",
    "for desc in descriptions:\n",
    "    try:\n",
    "        words = len(desc)\n",
    "        if words > max_words:\n",
    "            max_words = words\n",
    "    except:\n",
    "        pass\n",
    "print('Максимальное количество слов в самом длинном тексте: {} слов'.format(max_words))\n",
    "\n",
    "maxSequenceLength = max_words\n",
    "\n",
    "t = Tokenizer()\n",
    "    \n",
    "t.fit_on_texts(descriptions)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_docs_train = t.texts_to_sequences(x_train)\n",
    "encoded_docs_test = t.texts_to_sequences(x_test)\n",
    "padded_docs_train = sequence.pad_sequences(encoded_docs_train, maxlen=maxSequenceLength)\n",
    "padded_docs_test = sequence.pad_sequences(encoded_docs_test, maxlen=maxSequenceLength)\n",
    "\n",
    "total_unique_words = len(t.word_counts)\n",
    "print('Всего уникальных слов в словаре: {}'.format(total_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embdict[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "        #print(i)\n",
    "        #print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 3257, 300)         7076100   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 7,263,238\n",
      "Trainable params: 187,138\n",
      "Non-trainable params: 7,076,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False))\n",
    "#model.add(e)\n",
    "#e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)\n",
    "#model.add(e)\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(20, activation='sigmoid'))\n",
    "#model.add(Dropout=0.5)\n",
    "#model.add(Embedding(300, maxSequenceLength))\n",
    "#model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "#model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "#model.add(Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.4, recurrent_dropout=0.2)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# compile the model\n",
    "#rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "#model.compile(optimizer = rmsprop, loss = 'mean_squared_error', metrics=['mean_squared_error', 'mae'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20482 samples, validate on 2256 samples\n",
      "Epoch 1/5\n",
      " - 7948s - loss: 0.5779 - acc: 0.6920 - val_loss: 0.5808 - val_acc: 0.6946\n",
      "Epoch 2/5\n",
      " - 7942s - loss: 0.5765 - acc: 0.6892 - val_loss: 0.5863 - val_acc: 0.6724\n",
      "Epoch 3/5\n",
      " - 7947s - loss: 0.5746 - acc: 0.6916 - val_loss: 0.5752 - val_acc: 0.6835\n",
      "Epoch 4/5\n",
      " - 7952s - loss: 0.5717 - acc: 0.6961 - val_loss: 0.5738 - val_acc: 0.6924\n",
      "Epoch 5/5\n",
      " - 7959s - loss: 0.5701 - acc: 0.6960 - val_loss: 0.5724 - val_acc: 0.6977\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_docs_train, y_train, epochs = 5, verbose=2, validation_data=(padded_docs_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '5ep_1l_69n_full_vocab5-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ors/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ors/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ors/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = np.argmax(model.predict(x_test), axis=1)\n",
    "answer = np.argmax(y_test, axis=1)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, answer)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, answer, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, answer, average=\"macro\")*100))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
