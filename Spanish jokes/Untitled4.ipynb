{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ors/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import numpy as  np\n",
    "import os, random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "model = r\"SBW-vectors-300-min5.txt\"\n",
    "train=r\"./data/haha_2019_train_preprocessed_lemmatized.csv\"\n",
    "test=r\"./data/haha_2019_test_preprocessed_lemmatized.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) \n",
    "    text = re.sub('[¡¿.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w)>=3]\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    text=' '.join(words)\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(i) for i in tokens]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntexts=df[1].tolist()\\nscores=df[9].tolist()\\n\\ncategories=[1 if str(s)!='nan' else 0 for s in scores]\\n\\ndf = pd.read_csv(test, sep=',', header=None)\\ntexts_test=df[1].tolist()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read csv\n",
    "\n",
    "values= pd.read_csv(train, sep=',', header=None, encoding = 'utf-8-sig').values\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(values)\n",
    "#df=pd.DataFrame(values)\n",
    "\n",
    "m = len(values)\n",
    "\n",
    "train_length = int(0.9 * m)\n",
    "train_data, test_data = values[:train_length], values[train_length:]\n",
    "\n",
    "df=pd.DataFrame(train_data)\n",
    "\n",
    "texts_train=df[1].tolist()\n",
    "scores_train=df[9].tolist()\n",
    "categories_train=[1 if str(s)!='nan' else 0 for s in scores_train]\n",
    "\n",
    "df=pd.DataFrame(test_data)\n",
    "\n",
    "texts_test=df[1].tolist()\n",
    "scores_test=df[9].tolist()\n",
    "categories_test=[1 if str(s)!='nan' else 0 for s in scores_test]\n",
    "'''\n",
    "texts=df[1].tolist()\n",
    "scores=df[9].tolist()\n",
    "\n",
    "categories=[1 if str(s)!='nan' else 0 for s in scores]\n",
    "\n",
    "df = pd.read_csv(test, sep=',', header=None)\n",
    "texts_test=df[1].tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 15613\n"
     ]
    }
   ],
   "source": [
    "words=set()#set of all words\n",
    "for text in texts_train:\n",
    "    words_text=clean_text(text)\n",
    "    words.update(words_text)\n",
    "print(\"number of words: {0}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100000\n",
      "iteration 200000\n",
      "iteration 300000\n",
      "iteration 400000\n",
      "iteration 500000\n",
      "iteration 600000\n",
      "iteration 700000\n",
      "iteration 800000\n",
      "iteration 900000\n",
      "iteration 1000000\n",
      "size of dictionary: 11689\n"
     ]
    }
   ],
   "source": [
    "embdict=dict()#dictionary for words and emb\n",
    "index=0\n",
    "\n",
    "with open(model,'r',encoding = 'utf-8-sig')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word=str(f.readline()).replace('b','').replace('\\'','').replace('\\\\n','').lower().split()\n",
    "        w = stemmer.stem(word[0])\n",
    "        if w in words:\n",
    "            word.remove(word[0])\n",
    "            emb = [float(x) for x in word]\n",
    "            embdict[str(w)]=emb\n",
    "        index+=1\n",
    "        if index%100000==0:\n",
    "            print(\"iteration \"+str(index))\n",
    "\n",
    "print(\"size of dictionary: {0}\".format(len(embdict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "предобработка текста\n",
      "Максимальное количество слов в самом длинном тексте: 654 слов\n",
      "Всего уникальных слов в словаре: 15611\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "prep_texts_train =[]\n",
    "prep_texts_test =[]\n",
    "\n",
    "print(\"предобработка текста\")\n",
    "for t in texts_train:\n",
    "    prep_texts_train.append(' '.join(clean_text(t)))\n",
    "for t in texts_test:\n",
    "    prep_texts_test.append(' '.join(clean_text(t)))\n",
    "    \n",
    "descriptions = prep_texts_train\n",
    "    \n",
    "x_train = prep_texts_train\n",
    "y_train = categories_train\n",
    "    \n",
    "x_test = prep_texts_test\n",
    "y_test = categories_test\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "max_words = 0\n",
    "for desc in descriptions:\n",
    "    try:\n",
    "        words = len(desc)\n",
    "        if words > max_words:\n",
    "            max_words = words\n",
    "    except:\n",
    "        pass\n",
    "print('Максимальное количество слов в самом длинном тексте: {} слов'.format(max_words))\n",
    "\n",
    "maxSequenceLength = max_words\n",
    "\n",
    "t = Tokenizer()\n",
    "    \n",
    "t.fit_on_texts(descriptions)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_docs_train = t.texts_to_sequences(x_train)\n",
    "encoded_docs_test = t.texts_to_sequences(x_test)\n",
    "padded_docs_train = sequence.pad_sequences(encoded_docs_train, maxlen=maxSequenceLength)\n",
    "padded_docs_test = sequence.pad_sequences(encoded_docs_test, maxlen=maxSequenceLength)\n",
    "\n",
    "total_unique_words = len(t.word_counts)\n",
    "print('Всего уникальных слов в словаре: {}'.format(total_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embdict[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "        #print(i)\n",
    "        #print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 654, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 487,138\n",
      "Trainable params: 187,138\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False))\n",
    "#model.add(e)\n",
    "#e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)\n",
    "#model.add(e)\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(20, activation='sigmoid'))\n",
    "#model.add(Dropout=0.5)\n",
    "#model.add(Embedding(300, maxSequenceLength))\n",
    "#model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "#model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.2)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# compile the model\n",
    "#rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "#model.compile(optimizer = rmsprop, loss = 'mean_squared_error', metrics=['mean_squared_error', 'mae'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19199 samples, validate on 4800 samples\n",
      "Epoch 1/10\n",
      " - 1309s - loss: 0.5541 - acc: 0.7099 - val_loss: 0.5440 - val_acc: 0.7171\n",
      "Epoch 2/10\n",
      " - 1297s - loss: 0.5482 - acc: 0.7114 - val_loss: 0.5393 - val_acc: 0.7223\n",
      "Epoch 3/10\n",
      " - 1297s - loss: 0.5447 - acc: 0.7171 - val_loss: 0.5367 - val_acc: 0.7167\n",
      "Epoch 4/10\n",
      " - 1294s - loss: 0.5391 - acc: 0.7198 - val_loss: 0.5318 - val_acc: 0.7302\n",
      "Epoch 5/10\n",
      " - 1299s - loss: 0.5377 - acc: 0.7236 - val_loss: 0.5390 - val_acc: 0.7108\n",
      "Epoch 6/10\n",
      " - 1296s - loss: 0.5347 - acc: 0.7242 - val_loss: 0.5315 - val_acc: 0.7294\n",
      "Epoch 7/10\n",
      " - 1302s - loss: 0.5303 - acc: 0.7288 - val_loss: 0.5270 - val_acc: 0.7265\n",
      "Epoch 8/10\n",
      " - 1300s - loss: 0.5274 - acc: 0.7313 - val_loss: 0.5244 - val_acc: 0.7269\n",
      "Epoch 9/10\n",
      " - 1298s - loss: 0.5244 - acc: 0.7332 - val_loss: 0.5232 - val_acc: 0.7333\n",
      "Epoch 10/10\n",
      " - 1300s - loss: 0.5212 - acc: 0.7388 - val_loss: 0.5233 - val_acc: 0.7277\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_docs_train, y_train, epochs = 20, verbose=2, validation_data=(padded_docs_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = np.argmax(model.predict(x_test), axis=1)\n",
    "answer = np.argmax(y_test, axis=1)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, answer)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, answer, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, answer, average=\"macro\")*100))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
