{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn, tensorflow.keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "model = r\"SBW-vectors-300-min5.txt\"\n",
    "train=r\"haha_2019_train_preprocessed_lemmatized.csv\"\n",
    "test=r\"haha_2019_test_preprocessed_lemmatized.csv\"\n",
    "val1=r\"train.csv\"\n",
    "val2=r\"test.csv\"\n",
    "#train_sent1=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\data\\haha_2019_sent_politics_preprocessed_lemmatized.csv\"\n",
    "#train_sent2=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\data\\haha_2019_sent_preprocessed_lemmatized.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) \n",
    "    text = re.sub('[¡¿.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w)>=3]\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    text=' '.join(words)\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(i) for i in tokens]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "values= pd.read_csv(train, sep=',', header=None, encoding = 'utf-8-sig').values\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(values)\n",
    "#df=pd.DataFrame(values)\n",
    "\n",
    "m = len(values)\n",
    "\n",
    "train_length = int(0.9 * m)\n",
    "train_data, test_data = values[:train_length], values[train_length:]\n",
    "\n",
    "df=pd.DataFrame(train_data)\n",
    "\n",
    "texts_train=df[1].tolist()\n",
    "scores_train=df[9].tolist()\n",
    "categories_train_raw = [1 if str(s)!='nan' else 0 for s in scores_train]\n",
    "\n",
    "df=pd.DataFrame(test_data)\n",
    "\n",
    "texts_test=df[1].tolist()\n",
    "texts_test_original=df[1].tolist()\n",
    "scores_test=df[9].tolist()\n",
    "categories_test_raw = [1 if str(s)!='nan' else 0 for s in scores_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(test, sep=',', header=None, encoding = 'utf-8-sig')\n",
    "texts_ev=df[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "values1= pd.read_csv(val1, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "values1=scaler.fit_transform(values1)\n",
    "#values1=scaler.transform(values1)\n",
    "#values1=np.hstack((tfidf_train,values1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "values2= pd.read_csv(val2, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "values2=scaler.transform(values2)\n",
    "#values2=np.hstack((tfidf_test,values2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 20117\n"
     ]
    }
   ],
   "source": [
    "words=set()#set of all words\n",
    "for text in texts_train:\n",
    "    words_text=clean_text(text)\n",
    "    words.update(words_text)\n",
    "for text in texts_test:\n",
    "    words_text=clean_text(text)\n",
    "    words.update(words_text)\n",
    "for text in texts_ev:\n",
    "    words_text=clean_text(text)\n",
    "    words.update(words_text)\n",
    "print(\"number of words: {0}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100000\n",
      "iteration 200000\n",
      "iteration 300000\n",
      "iteration 400000\n",
      "iteration 500000\n",
      "iteration 600000\n",
      "iteration 700000\n",
      "iteration 800000\n",
      "iteration 900000\n",
      "iteration 1000000\n",
      "size of dictionary: 14423\n"
     ]
    }
   ],
   "source": [
    "embdict=dict()\n",
    "index=0\n",
    "\n",
    "with open(model,'r',encoding = 'utf-8-sig')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word=str(f.readline()).replace('b','').replace('\\'','').replace('\\\\n','').lower().split()\n",
    "        w = stemmer.stem(word[0])\n",
    "        if w in words:\n",
    "            word.remove(word[0])\n",
    "            emb = [float(x) for x in word]\n",
    "            embdict[str(w)]=emb\n",
    "        index+=1\n",
    "        if index%100000==0:\n",
    "            print(\"iteration \"+str(index))\n",
    "\n",
    "print(\"size of dictionary: {0}\".format(len(embdict)))\n",
    "del(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29530 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(texts_train+texts_test+texts_ev)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((29531, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embdict[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "        #print(i)\n",
    "        #print(word)\n",
    "del(embdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (21599, 100)\n",
      "Shape of data tensor: (2400, 100)\n",
      "Shape of data tensor: (5999, 100)\n"
     ]
    }
   ],
   "source": [
    "texts_train = tokenizer.texts_to_sequences(texts_train)\n",
    "texts_train = sequence.pad_sequences(texts_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', texts_train.shape)\n",
    "texts_test = tokenizer.texts_to_sequences(texts_test)\n",
    "texts_test = sequence.pad_sequences(texts_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', texts_test.shape)\n",
    "texts_ev = tokenizer.texts_to_sequences(texts_ev)\n",
    "texts_ev = sequence.pad_sequences(texts_ev, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', texts_ev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(21599, 100)\n",
      "(21599, 69)\n",
      "1000653\n",
      "(29531, 300)\n"
     ]
    }
   ],
   "source": [
    "print(len(texts_train[0]))\n",
    "print(texts_train.shape)\n",
    "print(values1.shape)\n",
    "print(vocab_size)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "categories_train = tf.keras.utils.to_categorical(categories_train_raw, num_classes)\n",
    "categories_test = tf.keras.utils.to_categorical(categories_test_raw, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21599, 100)\n",
      "(2400, 100)\n",
      "(21599, 2)\n",
      "(2400, 2)\n"
     ]
    }
   ],
   "source": [
    "print(texts_train.shape)\n",
    "print(texts_test.shape)\n",
    "print(categories_train.shape)\n",
    "print(categories_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 100, 300)     8859300     input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 128)     186880      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          98816       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 69)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 197)          0           bidirectional_2[0][0]            \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 64)           12672       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 64)           4160        dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 2)            130         dense_34[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,161,958\n",
      "Trainable params: 302,658\n",
      "Non-trainable params: 8,859,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputA=Input(shape=(100,))\n",
    "inputB=Input(shape=(len(values1[0]),))\n",
    "\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=100, trainable=False)(inputA)\n",
    "x = Bidirectional(LSTM(64, dropout=0.4, recurrent_dropout=0.2, return_sequences=True))(x)\n",
    "x = Bidirectional(LSTM(64, dropout=0.4, recurrent_dropout=0.2))(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "combined=concatenate([x.output, inputB])\n",
    "z=Dense(64, activation='relu')(combined)\n",
    "z=Dense(64, activation='relu')(z)\n",
    "z=Dense(2, activation='softmax')(z)\n",
    "\n",
    "model = tensorflow.keras.models.Model(inputs=[inputA, inputB], outputs=z)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1],\n",
    "              )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21599 samples, validate on 2400 samples\n",
      "Epoch 1/50\n",
      "21599/21599 [==============================] - 428s 20ms/sample - loss: 0.4555 - acc: 0.7792 - f1: 0.7792 - val_loss: 0.4619 - val_acc: 0.7842 - val_f1: 0.7842\n",
      "Epoch 2/50\n",
      "21599/21599 [==============================] - 424s 20ms/sample - loss: 0.4533 - acc: 0.7818 - f1: 0.7818 - val_loss: 0.4641 - val_acc: 0.7783 - val_f1: 0.7783\n",
      "Epoch 3/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4533 - acc: 0.7811 - f1: 0.7811 - val_loss: 0.4584 - val_acc: 0.7875 - val_f1: 0.7875\n",
      "Epoch 4/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4504 - acc: 0.7838 - f1: 0.7838 - val_loss: 0.4727 - val_acc: 0.7783 - val_f1: 0.7783\n",
      "Epoch 5/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4508 - acc: 0.7833 - f1: 0.7833 - val_loss: 0.4615 - val_acc: 0.7812 - val_f1: 0.7812\n",
      "Epoch 6/50\n",
      "21599/21599 [==============================] - 425s 20ms/sample - loss: 0.4476 - acc: 0.7851 - f1: 0.7851 - val_loss: 0.4669 - val_acc: 0.7758 - val_f1: 0.7758\n",
      "Epoch 7/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4461 - acc: 0.7856 - f1: 0.7856 - val_loss: 0.4602 - val_acc: 0.7892 - val_f1: 0.7892\n",
      "Epoch 8/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4440 - acc: 0.7862 - f1: 0.7862 - val_loss: 0.4589 - val_acc: 0.7842 - val_f1: 0.7842\n",
      "Epoch 9/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4416 - acc: 0.7861 - f1: 0.7861 - val_loss: 0.4624 - val_acc: 0.7746 - val_f1: 0.7746\n",
      "Epoch 10/50\n",
      "21599/21599 [==============================] - 425s 20ms/sample - loss: 0.4415 - acc: 0.7892 - f1: 0.7892 - val_loss: 0.4615 - val_acc: 0.7808 - val_f1: 0.7808\n",
      "Epoch 11/50\n",
      "21599/21599 [==============================] - 425s 20ms/sample - loss: 0.4404 - acc: 0.7913 - f1: 0.7913 - val_loss: 0.4563 - val_acc: 0.7912 - val_f1: 0.7912\n",
      "Epoch 12/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4398 - acc: 0.7917 - f1: 0.7917 - val_loss: 0.4601 - val_acc: 0.7858 - val_f1: 0.7858\n",
      "Epoch 13/50\n",
      "21599/21599 [==============================] - 425s 20ms/sample - loss: 0.4367 - acc: 0.7924 - f1: 0.7924 - val_loss: 0.4631 - val_acc: 0.7825 - val_f1: 0.7825\n",
      "Epoch 14/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4350 - acc: 0.7926 - f1: 0.7926 - val_loss: 0.4602 - val_acc: 0.7821 - val_f1: 0.7821\n",
      "Epoch 15/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4324 - acc: 0.7917 - f1: 0.7918 - val_loss: 0.4645 - val_acc: 0.7875 - val_f1: 0.7875\n",
      "Epoch 16/50\n",
      "21599/21599 [==============================] - 424s 20ms/sample - loss: 0.4322 - acc: 0.7942 - f1: 0.7941 - val_loss: 0.4593 - val_acc: 0.7867 - val_f1: 0.7867\n",
      "Epoch 17/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4278 - acc: 0.7969 - f1: 0.7969 - val_loss: 0.4630 - val_acc: 0.7808 - val_f1: 0.7808\n",
      "Epoch 18/50\n",
      "21599/21599 [==============================] - 427s 20ms/sample - loss: 0.4254 - acc: 0.7983 - f1: 0.7983 - val_loss: 0.4679 - val_acc: 0.7792 - val_f1: 0.7792\n",
      "Epoch 19/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4250 - acc: 0.7958 - f1: 0.7958 - val_loss: 0.4625 - val_acc: 0.7825 - val_f1: 0.7825\n",
      "Epoch 20/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4244 - acc: 0.7974 - f1: 0.7974 - val_loss: 0.4618 - val_acc: 0.7875 - val_f1: 0.7875\n",
      "Epoch 21/50\n",
      "21599/21599 [==============================] - 427s 20ms/sample - loss: 0.4215 - acc: 0.7982 - f1: 0.7982 - val_loss: 0.4668 - val_acc: 0.7817 - val_f1: 0.7817\n",
      "Epoch 22/50\n",
      "21599/21599 [==============================] - 428s 20ms/sample - loss: 0.4206 - acc: 0.7999 - f1: 0.7999 - val_loss: 0.4642 - val_acc: 0.7825 - val_f1: 0.7825\n",
      "Epoch 23/50\n",
      "21599/21599 [==============================] - 428s 20ms/sample - loss: 0.4179 - acc: 0.8009 - f1: 0.8009 - val_loss: 0.4717 - val_acc: 0.7792 - val_f1: 0.7792\n",
      "Epoch 24/50\n",
      "21599/21599 [==============================] - 427s 20ms/sample - loss: 0.4153 - acc: 0.8012 - f1: 0.8012 - val_loss: 0.4626 - val_acc: 0.7858 - val_f1: 0.7858\n",
      "Epoch 25/50\n",
      "21599/21599 [==============================] - 425s 20ms/sample - loss: 0.4126 - acc: 0.8025 - f1: 0.8025 - val_loss: 0.4657 - val_acc: 0.7825 - val_f1: 0.7825\n",
      "Epoch 26/50\n",
      "21599/21599 [==============================] - 426s 20ms/sample - loss: 0.4132 - acc: 0.8040 - f1: 0.8040 - val_loss: 0.4832 - val_acc: 0.7783 - val_f1: 0.7783\n",
      "Epoch 27/50\n",
      "21599/21599 [==============================] - 424s 20ms/sample - loss: 0.4093 - acc: 0.8049 - f1: 0.8049 - val_loss: 0.4716 - val_acc: 0.7808 - val_f1: 0.7808\n",
      "Epoch 28/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.4072 - acc: 0.8066 - f1: 0.8066 - val_loss: 0.4697 - val_acc: 0.7867 - val_f1: 0.7867\n",
      "Epoch 29/50\n",
      "21599/21599 [==============================] - 424s 20ms/sample - loss: 0.4038 - acc: 0.8079 - f1: 0.8079 - val_loss: 0.4852 - val_acc: 0.7804 - val_f1: 0.7804\n",
      "Epoch 30/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.4031 - acc: 0.8066 - f1: 0.8066 - val_loss: 0.4830 - val_acc: 0.7808 - val_f1: 0.7808\n",
      "Epoch 31/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.4008 - acc: 0.8116 - f1: 0.8116 - val_loss: 0.4815 - val_acc: 0.7825 - val_f1: 0.7825\n",
      "Epoch 32/50\n",
      "21599/21599 [==============================] - 422s 20ms/sample - loss: 0.3983 - acc: 0.8098 - f1: 0.8098 - val_loss: 0.4779 - val_acc: 0.7779 - val_f1: 0.7779\n",
      "Epoch 33/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3947 - acc: 0.8124 - f1: 0.8123 - val_loss: 0.4740 - val_acc: 0.7808 - val_f1: 0.7808\n",
      "Epoch 34/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3919 - acc: 0.8134 - f1: 0.8134 - val_loss: 0.4827 - val_acc: 0.7821 - val_f1: 0.7821\n",
      "Epoch 35/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3907 - acc: 0.8170 - f1: 0.8170 - val_loss: 0.4904 - val_acc: 0.7754 - val_f1: 0.7754\n",
      "Epoch 36/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3865 - acc: 0.8183 - f1: 0.8183 - val_loss: 0.5050 - val_acc: 0.7742 - val_f1: 0.7742\n",
      "Epoch 37/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3862 - acc: 0.8196 - f1: 0.8196 - val_loss: 0.4917 - val_acc: 0.7763 - val_f1: 0.7762\n",
      "Epoch 38/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3830 - acc: 0.8182 - f1: 0.8182 - val_loss: 0.5159 - val_acc: 0.7742 - val_f1: 0.7742\n",
      "Epoch 39/50\n",
      "21599/21599 [==============================] - 422s 20ms/sample - loss: 0.3824 - acc: 0.8193 - f1: 0.8192 - val_loss: 0.4995 - val_acc: 0.7817 - val_f1: 0.7817\n",
      "Epoch 40/50\n",
      "21599/21599 [==============================] - 422s 20ms/sample - loss: 0.3761 - acc: 0.8230 - f1: 0.8230 - val_loss: 0.5170 - val_acc: 0.7671 - val_f1: 0.7671\n",
      "Epoch 41/50\n",
      "21599/21599 [==============================] - 422s 20ms/sample - loss: 0.3755 - acc: 0.8235 - f1: 0.8235 - val_loss: 0.5032 - val_acc: 0.7742 - val_f1: 0.7742\n",
      "Epoch 42/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3739 - acc: 0.8240 - f1: 0.8240 - val_loss: 0.4960 - val_acc: 0.7758 - val_f1: 0.7758\n",
      "Epoch 43/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3735 - acc: 0.8237 - f1: 0.8237 - val_loss: 0.5002 - val_acc: 0.7804 - val_f1: 0.7804\n",
      "Epoch 44/50\n",
      "21599/21599 [==============================] - 422s 20ms/sample - loss: 0.3706 - acc: 0.8266 - f1: 0.8266 - val_loss: 0.5221 - val_acc: 0.7733 - val_f1: 0.7733\n",
      "Epoch 45/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3671 - acc: 0.8278 - f1: 0.8278 - val_loss: 0.5062 - val_acc: 0.7808 - val_f1: 0.7808\n",
      "Epoch 46/50\n",
      "21599/21599 [==============================] - 422s 20ms/sample - loss: 0.3632 - acc: 0.8281 - f1: 0.8281 - val_loss: 0.5239 - val_acc: 0.7779 - val_f1: 0.7779\n",
      "Epoch 47/50\n",
      "21599/21599 [==============================] - 422s 20ms/sample - loss: 0.3601 - acc: 0.8304 - f1: 0.8304 - val_loss: 0.5202 - val_acc: 0.7683 - val_f1: 0.7683\n",
      "Epoch 48/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3602 - acc: 0.8323 - f1: 0.8323 - val_loss: 0.5168 - val_acc: 0.7763 - val_f1: 0.7762\n",
      "Epoch 49/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3548 - acc: 0.8341 - f1: 0.8341 - val_loss: 0.5251 - val_acc: 0.7758 - val_f1: 0.7758\n",
      "Epoch 50/50\n",
      "21599/21599 [==============================] - 423s 20ms/sample - loss: 0.3552 - acc: 0.8355 - f1: 0.8355 - val_loss: 0.5205 - val_acc: 0.7638 - val_f1: 0.7637\n",
      "F1-score: 74.620648\n"
     ]
    }
   ],
   "source": [
    "model.fit([texts_train, np.array(values1)], \n",
    "          categories_train, epochs=50, \n",
    "          verbose=1, \n",
    "          validation_data=([texts_test, np.array(values2)], categories_test)\n",
    "         )\n",
    "#           callbacks=callbacks)\n",
    "predict = np.argmax(model.predict([np.array(texts_test),np.array(values2)]), axis=1)\n",
    "answer = np.argmax(categories_test, axis=1)\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 72.696562\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('tfidf_multitask_add_82_79.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
