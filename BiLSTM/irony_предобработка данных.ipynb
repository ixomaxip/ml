{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ors/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ors/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import numpy as  np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "path_model = r\"./word2vec_twitter_model.bin\"\n",
    "path_data=r\"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nm_train.txt\n",
      "np_val.txt\n",
      "p_test.txt\n",
      "i_test.txt\n",
      "ni_train.txt\n",
      "np_train.txt\n",
      "nm_val.txt\n",
      "ni_test.txt\n",
      "i_val.txt\n",
      "np_test.txt\n",
      "nm_test.txt\n",
      "none-class-1200.txt\n",
      "p_val.txt\n",
      "m_train.txt\n",
      "i_train.txt\n",
      "csv\n",
      "[Errno 21] Is a directory: './data/csv'\n",
      "m_val.txt\n",
      "m_test.txt\n",
      "p_train.txt\n",
      "ni_val.txt\n",
      "Всего слов: 10666\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) \n",
    "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w)>3]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    text=' '.join(words)\n",
    "    tokens = word_tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    return stemmed\n",
    "    \n",
    "words=set()#множество слов по всем текстам\n",
    "\n",
    "for f in os.listdir(path_data):\n",
    "    print(f)\n",
    "    full_path = path_data+'/'+f\n",
    "    try:\n",
    "        f_text = open(full_path, \"r\").readlines()\n",
    "        for line in f_text:\n",
    "            words_text=clean_text(line)\n",
    "            words.update(words_text)\n",
    "    except Exception as  err:\n",
    "        print(err)\n",
    "        \n",
    "        pass\n",
    "        '''\n",
    "        lines=[]\n",
    "        with open(full_path) as file:\n",
    "            while True:\n",
    "                try:\n",
    "                    line = file.readline()\n",
    "                    lines.append(line)\n",
    "                except:\n",
    "                    pass\n",
    "        for line in lines:\n",
    "            words_text=clean_text(line)\n",
    "            words.update(words_text)\n",
    "        '''\n",
    "    \n",
    "print(\"Всего слов: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ors/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000000\n",
      "iteration 2000000\n",
      "iteration 3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embdict=dict()#словарь эмбеддингов и слов\n",
    "index=0\n",
    "porter = PorterStemmer()\n",
    "\n",
    "with open(path_model,'rb')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode(errors='ignore')\n",
    "            if ch ==' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if len(word) != 0:\n",
    "            tp= np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            word = porter.stem(word.lower())\n",
    "            if word in words:\n",
    "                embdict[str(word)]=tp.tolist()\n",
    "\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "        index+=1\n",
    "        if index%1000000==0:\n",
    "            print(\"iteration \"+str(index))\n",
    "print(embdict)\n",
    "print(\"Слов в словаре:\"+str(len(embdict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nm_train.txt\n",
      "np_val.txt\n",
      "p_test.txt\n",
      "i_test.txt\n",
      "ni_train.txt\n",
      "np_train.txt\n",
      "nm_val.txt\n",
      "ni_test.txt\n",
      "i_val.txt\n",
      "np_test.txt\n",
      "nm_test.txt\n",
      "none-class-1200.txt\n",
      "p_val.txt\n",
      "m_train.txt\n",
      "i_train.txt\n",
      "csv\n",
      "m_val.txt\n",
      "m_test.txt\n",
      "p_train.txt\n",
      "ni_val.txt\n",
      "Всего слов: 10666\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    print(f)\n",
    "    full_path = path_data+'/'+f\n",
    "    try:\n",
    "        f_text = open(full_path, \"r\").readlines()\n",
    "        vectors=[]\n",
    "        lines=[]\n",
    "        for line in f_text:\n",
    "            words_text=clean_text(line)\n",
    "            emb_vector=np.zeros(400)\n",
    "            for word in words_text:\n",
    "                try:\n",
    "                    emb_vector+=embdict[word]\n",
    "                except:\n",
    "                    pass\n",
    "            lines.append(line)\n",
    "            vectors.append(emb_vector)\n",
    "        df=pd.DataFrame(vectors)\n",
    "        df.insert(loc=0, column='texts', value=lines)\n",
    "        df.to_csv(f+'.csv',sep=';')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"Всего слов: \"+str(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>дальше код про сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras import utils\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_train1=r\"none-class-1200.txt.csv\"\n",
    "name_train2=r\"i_train.txt.csv\"\n",
    "name_test=r\"i_test.txt.csv\"\n",
    "name_val=r\"p_val.txt.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(name_train1, encoding='utf-8') as f:\n",
    "        df1 = pd.read_csv(f, sep=';', header=0, decimal = '.', index_col=0)\n",
    "with open(name_train2, encoding='utf-8') as f:\n",
    "        df2 = pd.read_csv(f, sep=';', header=0, decimal = '.', index_col=0)       \n",
    "\n",
    "train_data1 = df1.values\n",
    "train_data2 = df2.values\n",
    "\n",
    "train_data1 = np.column_stack((np.zeros(len(train_data1)),train_data1))\n",
    "train_data2 = np.column_stack((np.ones(len(train_data2)),train_data2))\n",
    "\n",
    "train_data=np.vstack((train_data1,train_data2)) \n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "with open(name_test, encoding='utf-8') as f:\n",
    "        df1 = pd.read_csv(f, sep=';', header=0, decimal = '.', index_col=0)\n",
    "with open(name_val, encoding='utf-8') as f:\n",
    "        df2 = pd.read_csv(f, sep=';', header=0, decimal = '.', index_col=0)\n",
    "test_data = df1.values\n",
    "test_data = np.column_stack((np.ones(len(test_data)),test_data))\n",
    "val_data = df2.values\n",
    "val_data = np.column_stack((np.ones(len(val_data)),val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_floats(texts, categories, vectors):\n",
    "    _texts=[]\n",
    "    _categories=[]\n",
    "    _vectors=[]\n",
    "    for i in range(len(texts)):\n",
    "        if type(texts[i]) is str:\n",
    "            _texts.append(clean_text(texts[i]))\n",
    "            _categories.append(categories[i])\n",
    "            _vectors.append(vectors[i])\n",
    "    return _texts,_categories,_vectors\n",
    "\n",
    "df = pd.DataFrame(train_data)\n",
    "texts = df[1].tolist()\n",
    "categories = df[0].tolist()\n",
    "vectors = df.drop([0,1], axis=1).values\n",
    "texts,categories,vectors=remove_floats(texts,categories,vectors)\n",
    "\n",
    "df = pd.DataFrame(test_data)\n",
    "texts_test = df[1].tolist()\n",
    "categories_test = df[0].tolist()\n",
    "vectors_test = df.drop([0,1], axis=1).values\n",
    "texts_test,categories_test,vectors_test=remove_floats(texts_test,categories_test,vectors_test)\n",
    "\n",
    "df = pd.DataFrame(val_data)\n",
    "texts_val = df[1].tolist()\n",
    "categories_val = df[0].tolist()\n",
    "vectors_val = df.drop([0,1], axis=1).values\n",
    "texts_val,categories_val,vectors_val=remove_floats(texts_val,categories_val,vectors_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное количество слов в самом длинном тексте: 94 слов\n",
      "Всего уникальных слов в словаре: 5907\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "descriptions = texts+texts_test\n",
    "    \n",
    "x_train = texts\n",
    "y_train = categories\n",
    "    \n",
    "x_test = texts_test\n",
    "y_test = categories_test\n",
    "\n",
    "x_val = texts_val\n",
    "y_val = categories_val\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "max_words = 0\n",
    "for desc in descriptions:\n",
    "    try:\n",
    "        words = len(desc)\n",
    "        if words > max_words:\n",
    "            max_words = words\n",
    "    except:\n",
    "        pass\n",
    "print('Максимальное количество слов в самом длинном тексте: {} слов'.format(max_words))\n",
    "\n",
    "maxSequenceLength = max_words\n",
    "\n",
    "t = Tokenizer()\n",
    "    \n",
    "t.fit_on_texts(descriptions)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_docs_train = t.texts_to_sequences(x_train)\n",
    "encoded_docs_test = t.texts_to_sequences(x_test)\n",
    "encoded_docs_val = t.texts_to_sequences(x_val)\n",
    "padded_docs_train = sequence.pad_sequences(encoded_docs_train, maxlen=maxSequenceLength)\n",
    "padded_docs_test = sequence.pad_sequences(encoded_docs_test, maxlen=maxSequenceLength)\n",
    "padded_docs_val= sequence.pad_sequences(encoded_docs_val, maxlen=maxSequenceLength)\n",
    "\n",
    "total_unique_words = len(t.word_counts)\n",
    "print('Всего уникальных слов в словаре: {}'.format(total_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ve\n",
      "'ll\n",
      "'re\n",
      "gamerg\n",
      "peshawarattack\n",
      "ericgarn\n",
      "lecoincanard\n",
      "thanksobama\n",
      "torturereport\n",
      "sonyhack\n",
      "blacklivesmatt\n",
      "handsupdontshoot\n",
      "'the\n",
      "nfuller\n",
      "eatenal\n",
      "swormx\n",
      "anticonversionlaw\n",
      "finalsweek\n",
      "cfbplayoff\n",
      "mufflerman\n",
      "gharwapsi\n",
      "absolutetosh\n",
      "newyearnewm\n",
      "cameronmustgo\n",
      "todayimlov\n",
      "'republican\n",
      "'effect\n",
      "giveitup\n",
      "thisismiser\n",
      "nosocialconsci\n",
      "reallynotr\n",
      "nomealfitt\n",
      "studentloan\n",
      "intelsa\n",
      "pbuffkin\n",
      "shameony\n",
      "putyourtoysaway\n",
      "badboyfriend\n",
      "twittertim\n",
      "d|have\n",
      "tortugaroug\n",
      "stuism\n",
      "notjustic\n",
      "fantasticfriday\n",
      "thatscool\n",
      "whatmoredoesonene\n",
      "copycatsay\n",
      "michiganman\n",
      "sosnoff\n",
      "acatholicpray\n",
      "cgshore\n",
      "quiet|but\n",
      "convos|and\n",
      "weaknesses|todestroy\n",
      "ifussss\n",
      "'iron\n",
      "rexemoji\n",
      "manitibaprob\n",
      "meaningful|cough\n",
      "vlog|\n",
      "keepyourpromis\n",
      "truebro\n",
      "killerband\n",
      "ineednewboot\n",
      "racewar\n",
      "lovenotwar\n",
      "shouldofstayedinb\n",
      "toomuchlov\n",
      "'d\n",
      "onlykid\n",
      "goodtv\n",
      "freespeechmov\n",
      "kcroyal\n",
      "winterkil\n",
      "whatdoiexpect\n",
      "nmucomputersarethebest\n",
      "goodlead\n",
      "thirdyearlif\n",
      "'legendari\n",
      "soorganis\n",
      "denialcsgo\n",
      "nashuffl\n",
      "qarara\n",
      "justbrows\n",
      "typical|\n",
      "iaukea\n",
      "needanewroutin\n",
      "feelshit\n",
      "michaelbeeren\n",
      "ilookreallygood\n",
      "wishwehada\n",
      "milstrik\n",
      "communitychoic\n",
      "rannimarti\n",
      "nlprimetim\n",
      "serverlif\n",
      "mastre\n",
      "gonnahaveagoodday\n",
      "norespect\n",
      "ptikeptpakfirst\n",
      "mehrockingmyblumonsoont\n",
      "khajo\n",
      "pakschoolsieg\n",
      "indiawithpakistan\n",
      "whitewin\n",
      "chardsohard\n",
      "dunckin\n",
      "salgovernal\n",
      "kashmirboycott\n",
      "||north\n",
      "vencí\n",
      "umdfin\n",
      "cutestcoupl\n",
      "somepplhavetostudi\n",
      "palinspawn\n",
      "backofficeguru\n",
      "somadatbandhandadorama\n",
      "boldandbeauti\n",
      "goodgovernanceday\n",
      "acknowledgementisfirststep\n",
      "antisem\n",
      "instacraz\n",
      "thatsabossbitchrightther\n",
      "excessivecomsumpt\n",
      "isthatajok\n",
      "conerg\n",
      "bestnewartist\n",
      "ecusta\n",
      "suckitmothernatur\n",
      "bigbootybitch\n",
      "theygavemetcucolorsforchristma\n",
      "'fulli\n",
      "necrophel\n",
      "latinfin\n",
      "justajok\n",
      "intodeadth\n",
      "secularindia\n",
      "minnesotaproblem\n",
      "edryden\n",
      "perksofbeingasenior\n",
      "morecoffe\n",
      "freelesson\n",
      "notsarcasm\n",
      "timezoneproblem\n",
      "archiesday\n",
      "anychanceofasoci\n",
      "zenzontle\n",
      "vwigan\n",
      "'rim\n",
      "lotsofdepopupload\n",
      "dongiotravel\n",
      "thfloor\n",
      "thanksforthehelp\n",
      "webrtcpari\n",
      "sucksimissedy\n",
      "suchagoodweek\n",
      "nursinglif\n",
      "workingdaysandnight\n",
      "happymarriag\n",
      "myhappyfac\n",
      "responded|\n",
      "jakeplumm\n",
      "outsidethelin\n",
      "tyneweirsunday\n",
      "myrelationship\n",
      "moveitalong\n",
      "fbcjax\n",
      "feelingabandon\n",
      "skinnerbox\n",
      "blondess\n",
      "fakelobst\n",
      "lifestyleoftherichandfam\n",
      "worldofswed\n",
      "imissedu\n",
      "cancup\n",
      "decemberbessen\n",
      "badbusi\n",
      "moodmayb\n",
      "weatherbomb\n",
      "goodstarttotheday\n",
      "stopwatchingthenew\n",
      "andstoppreachingoffyoutub\n",
      "notfortheintellectu\n",
      "drhandsomedenni\n",
      "coldpant\n",
      "offtomysteriousplac\n",
      "emopostahead\n",
      "beautifulliar\n",
      "lovethwayyouli\n",
      "fazzanelli\n",
      "jeffroylat\n",
      "acuqir\n",
      "seax\n",
      "ohgoditsfridayagain\n",
      "itsdead\n",
      "notexcitedaboutthisshitatal\n",
      "almostdon\n",
      "itstruetho\n",
      "donttrustnobodi\n",
      "iwokeuplikethi\n",
      "stopcompla\n",
      "itsacurs\n",
      "yclnationalcongress\n",
      "hesnode\n",
      "pregnancyproblem\n",
      "'anoth\n",
      "'best\n",
      "gpabdriv\n",
      "ebayipad\n",
      "greatschedul\n",
      "incaseyoucanttel\n",
      "idontwanttobuildasnowman\n",
      "mommylif\n",
      "changethecanon\n",
      "aintitfun\n",
      "globalartisthma\n",
      "shamiwit\n",
      "sydneysieg\n",
      "eallyy\n",
      "coonti\n",
      "notworthit\n",
      "unlitigi\n",
      "alavsor\n",
      "nationalchampionship\n",
      "'import\n",
      "srslyofallroom\n",
      "noreligion\n",
      "ifitsr\n",
      "ptiukgonawazgocampaign||show\n",
      "guitarpick\n",
      "twittereantaboutlif\n",
      "|gift\n",
      "god|improv\n",
      "skills|\n",
      "karmicthought\n",
      "sodemocrat\n",
      "operaciónpandora\n",
      "wemissedyoutoo\n",
      "culturalmarx\n",
      "sofulfil\n",
      "examproblem\n",
      "storywasstupid\n",
      "hoursofcrazi\n",
      "poweroff\n",
      "whatisbirthcontrol\n",
      "bbgalad\n",
      "nasni\n",
      "lawenforc\n",
      "delhirap\n",
      "christmasshop\n",
      "||well\n",
      "ibeendoingintegralssinceth\n",
      "thgrade\n",
      "laundrymatfun\n",
      "yesisaidnot\n",
      "youspinmeround\n",
      "thatsmykid\n",
      "smusic\n",
      "teachingth\n",
      "ohbliss\n",
      "dayservic\n",
      "prioritymail\n",
      "nosuchth\n",
      "koadmedia\n",
      "arrowmidseasonfinal\n",
      "neverboxingdayshoppingagain\n",
      "bigclub\n",
      "theyllgiveyounightmar\n",
      "youaskedforit\n",
      "obish\n",
      "thinkpositive|\n",
      "bhootnaath\n",
      "lateforwork\n",
      "sticktopar\n",
      "interesinglif\n",
      "whatamidoingwithmylif\n",
      "longesthashtagnotneededbutyolo\n",
      "ilovebrighthous\n",
      "etownmil\n",
      "chemtrailsdontexist\n",
      "crafts|pleas\n",
      "doinitright\n",
      "maybenextyear\n",
      "newyearsresolut\n",
      "hatewhenthingsdontworkright\n",
      "nobelpeacepr\n",
      "thechicago\n",
      "workingpress\n",
      "sonotclassi\n",
      "everyonecanfli\n",
      "adelaideroutecancel\n",
      "nelza\n",
      "countonmoth\n",
      "wordsofkind\n",
      "gamergate'r\n",
      "watched'mak\n",
      "effroomm\n",
      "yourtooloud\n",
      "personalalarmclock\n",
      "notnecessari\n",
      "cuzididntwantgoodfood\n",
      "nightauditadventur\n",
      "resortjobprob\n",
      "'pro\n",
      "josk\n",
      "||spell\n",
      "panndder\n",
      "bloisolson\n",
      "soggybreek\n",
      "realmvp\n",
      "daibyday\n",
      "donwel\n",
      "ireallythinkyouareafuckingidiot\n",
      "coercivecontrol\n",
      "'skywalk\n",
      "'frivol\n",
      "legalizeit\n",
      "flzjinglebal\n",
      "smellya\n",
      "heavyamountsofsarcasm\n",
      "icebucketchalleng\n",
      "notinmywallet\n",
      "thanksfortheannoyingnot\n",
      "villalovein\n",
      "astlei\n",
      "ukvslou\n",
      "mcyuck\n",
      "lostsal\n",
      "nosleepdecemb\n",
      "motheroftheyear\n",
      "ortypecast\n",
      "'onli\n",
      "blampi\n",
      "costadelsol\n",
      "ilovecanada\n",
      "thingsyousaytoyourbestfriend\n",
      "allcelebritiesarefriend\n",
      "centeroftheunivers\n",
      "fergusonriottip\n",
      "stores||\n",
      "vitoandvito\n",
      "waarmedia\n",
      "ycot\n",
      "growingwint\n",
      "captaincourag\n",
      "oteiba\n",
      "cantwaitfortomorrow\n",
      "justwhatiw\n",
      "sickworld\n",
      "ihopeigetitback\n",
      "parisbreedenw\n",
      "racialprofil\n",
      "pizzawin\n",
      "timetodecor\n",
      "tistheseason\n",
      "coldhandswarmheart\n",
      "needmoremoney\n",
      "||it\n",
      "ursulaonamonday|\n",
      "wordshurt\n",
      "moneytho\n",
      "duleepsingh\n",
      "butnotr\n",
      "sheneverleft\n",
      "goodgrief\n",
      "dearmothica\n",
      "italktoomuchshit\n",
      "yourstillclockingmetho\n",
      "thatswhyyourgirlstillfollow\n",
      "servicewithasmil\n",
      "lkoch\n",
      "needthemoney\n",
      "restlessduncan\n",
      "bhopalaprayerforrain\n",
      "avnish\n",
      "ndtv||\n",
      "idpw\n",
      "beckthi\n",
      "strael\n",
      "dsebesta\n",
      "isibueno\n",
      "boohack\n",
      "cheapbastard\n",
      "worthashot\n",
      "policebrut\n",
      "themoreyouknow\n",
      "soeasytotakeapicwithatoddl\n",
      "wksandcount\n",
      "awhile|\n",
      "quammunist\n",
      "goodellmustgo\n",
      "tuskaloosa\n",
      "startonlineher\n",
      "justkiddingilovethem\n",
      "easypay\n",
      "deanbcfc\n",
      "wasalrahman\n",
      "betarock\n",
      "ceddamack\n",
      "jusygirlyth\n",
      "biggrooverecord\n",
      "leavemealonemom\n",
      "greatcustomerrel\n",
      "michaelmann\n",
      "'attack\n",
      "orcsliv\n",
      "guardiansofpeac\n",
      "actuallyihopeso\n",
      "butseriouslytheroadsarebad\n",
      "onuchapel\n",
      "freedomofspeech\n",
      "thegoldcoin\n",
      "youratruefriendtoh\n",
      "spnew\n",
      "factaboutabus\n",
      "quiksilvergoessuperson\n",
      "ijustwannasleep\n",
      "gotinterupt\n",
      "transmitterproblem\n",
      "noodlescen\n",
      "deadhenri\n",
      "bouncebackanni\n",
      "cantopeney\n",
      "bjameswe\n",
      "pearlburg\n",
      "bestroomieaward\n",
      "asongofic\n",
      "'all\n",
      "relationshipgoalsaccomplish\n",
      "flobeam\n",
      "theinterview\n",
      "homebasedbusi\n",
      "ilovelif\n",
      "shitbant\n",
      "noonegivesafuck\n",
      "allyouneedisstyl\n",
      "easyday\n",
      "serpentina\n",
      "tuesday~\n",
      "shelovesitwhenisendthatmani\n",
      "daystretch\n",
      "theteach\n",
      "youowem\n",
      "lyntoncrosbi\n",
      "weback\n",
      "rudebuttru\n",
      "'unfollow\n",
      "beforecoffe\n",
      "athensbyrain\n",
      "coolurstyl\n",
      "hensbyrian\n",
      "totallystillgoingtodrinktheredbul\n",
      "paycoin\n",
      "iwasshit\n",
      "seasonlif\n",
      "vmcorrupt\n",
      "thankyoumicrosoft\n",
      "startalloveragain\n",
      "|jihadi\n",
      "while|fratboy\n",
      "metahipst\n",
      "cosby|\n",
      "watcheronawal\n",
      "'do\n",
      "actuallysorri\n",
      "omwtothemet\n",
      "mathisfun\n",
      "hebetrip\n",
      "badassbraeden\n",
      "alwayswork\n",
      "lingaa\n",
      "youcanthearit\n",
      "generationi\n",
      "newwaytocommun\n",
      "deathtotaliban\n",
      "deathtoisi\n",
      "thang~\n",
      "principalswag\n",
      "iwriteth\n",
      "truestoryfam\n",
      "senioritisisr\n",
      "painewebb\n",
      "legitreasontocomplain\n",
      "orgabeh\n",
      "jepischk\n",
      "djdurkin\n",
      "nicejob\n",
      "lestertochicago\n",
      "slowinternet\n",
      "notthepoint\n",
      "modeltownnotforgotten\n",
      "gonawazgo\n",
      "serialbradi\n",
      "favth\n",
      "tvandsleep\n",
      "haugenma\n",
      "yesimreadingthisbook\n",
      "ilovemycat\n",
      "pawproject\n",
      "surgicalstock\n",
      "intreeg\n",
      "notstok\n",
      "factaboutdentist\n",
      "ｆｏｌｌｏｗ\n",
      "notoldschool\n",
      "iwanttogohom\n",
      "lambia\n",
      "ludicrousstori\n",
      "nohelp\n",
      "'western\n",
      "xringmiracl\n",
      "|colleg\n",
      "'till\n",
      "practicewhatyoupreach\n",
      "groceriesnotgun\n",
      "thingsbetterthantitansjag\n",
      "bigjoefavorit\n",
      "thebullscanhavehim\n",
      "busgaglia\n",
      "isntitiron\n",
      "donchathink\n",
      "imnotsur\n",
      "werleman\n",
      "thisisadisast\n",
      "hockeytourna\n",
      "reflif\n",
      "funnyguy\n",
      "handymedicalfact\n",
      "'arbitrari\n",
      "imgoingtostruggl\n",
      "nonrecur\n",
      "needmorecoffe\n",
      "scaredshitless\n",
      "imgettinold\n",
      "ionlygetbett\n",
      "stillgoingout\n",
      "ifagre\n",
      "onelovefestiv\n",
      "middlechildsyndrom\n",
      "d|no\n",
      "supportfawad\n",
      "web|pleas\n",
      "||firefox\n",
      "deathofm\n",
      "noagenda\n",
      "thisiscrap\n",
      "nothelpfulatal\n",
      "iliketogetdirti\n",
      "royex\n",
      "ideiasdebolosdocesedelicia\n",
      "whatasupris\n",
      "whatelseisnew\n",
      "lorraineel\n",
      "deeprespect\n",
      "'t\n",
      "valueformoneynot\n",
      "fuckuf\n",
      "givemeabrew\n",
      "oldjok\n",
      "soclosetilden\n",
      "illridewithy\n",
      "holdtheapplaus\n",
      "doomandgloom\n",
      "gimmesnow\n",
      "'holiday\n",
      "cjsinner\n",
      "onecolleen\n",
      "thanksfin\n",
      "rehydratewhenyadehydr\n",
      "enscomb\n",
      "aretheyseri\n",
      "stupidsauc\n",
      "mymomcoulddobett\n",
      "seewhatidid\n",
      "readpeopl\n",
      "poortast\n",
      "dontwanttobeher\n",
      "iamgiant\n",
      "single||it\n",
      "me||i\n",
      "chillytimbo\n",
      "brownbearmik\n",
      "greensfail\n",
      "aneyeforaney\n",
      "jeremynortham\n",
      "gosfordpark\n",
      "gymfanat\n",
      "euvat\n",
      "dontbelievemejustwatch\n",
      "liberalment\n",
      "||dri\n",
      "funchristma\n",
      "fucktherain\n",
      "wherearemyhunt\n",
      "imgonnalooksocutetoday\n",
      "delilyf\n",
      "somuchfun\n",
      "collegeiskillingm\n",
      "onemoreweek\n",
      "shiftworkerlif\n",
      "iwannagobacktob\n",
      "rbrnetwork\n",
      "fuckinsnow\n",
      "ironagethink\n",
      "itisachristmastre\n",
      "flattir\n",
      "alissamari\n",
      "kiddingnotkid\n",
      "sheseriouslyrock\n",
      "letmegohom\n",
      "parttimecook\n",
      "blackmossptc\n",
      "thenewsroom\n",
      "sermonis\n",
      "rulesweremadetobebroken\n",
      "here||\n",
      "everythingisfin\n",
      "hellonwheel\n",
      "toppscard\n",
      "fluday\n",
      "backupoff\n",
      "howmanygod\n",
      "punctuationmatt\n",
      "worstsongev\n",
      "sopitlikeitshot\n",
      "’\n",
      "nurkic\n",
      "inthisdimens\n",
      "dirtydeathdog\n",
      "bfhardlin\n",
      "|highland\n",
      "mentissu\n",
      "'sneez\n",
      "livingontheedg\n",
      "sccmadmininhealthcar\n",
      "friesenfamilychristma\n",
      "yastagh\n",
      "truegentlemen\n",
      "familyvac\n",
      "morefollowerspleas\n",
      "neverwork\n",
      "psychhumor\n",
      "muslimapolog\n",
      "youthiacrasi\n",
      "baltimoresun\n",
      "teamipswich\n",
      "itwasquiet\n",
      "beenshit\n",
      "maybetomorrow\n",
      "celebrityleakedphoto\n",
      "whereswint\n",
      "'protect\n",
      "fishsitt\n",
      "bewareofdog\n",
      "lauralobaugh\n",
      "funinthesun\n",
      "orisit\n",
      "~i\n",
      "dogmatic~\n",
      "sillybrendan\n",
      "mrjamieeast\n",
      "editorspick\n",
      "sillyvin\n",
      "thanksmom\n",
      "positivenot\n",
      "waynesworld\n",
      "ssarcasm\n",
      "thingsthatarewrong\n",
      "iwanttosleep\n",
      "cwmmate\n",
      "jimrom\n",
      "replec\n",
      "fitnessmotiv\n",
      "idiots|might\n",
      "inasong\n",
      "lifeal\n",
      "hellbefiredinelevenmonth\n",
      "lobbyinglif\n",
      "younger||\n",
      "bestchristmasev\n",
      "fthunter\n",
      "amdogfun\n",
      "dpeterson\n",
      "humanbrain\n",
      "henryisalegend\n",
      "philisinnoc\n",
      "howtogetunfollow\n",
      "slabour\n",
      "blairmcdoug\n",
      "deputydug\n",
      "ilovecolleg\n",
      "holleb\n",
      "psndown\n",
      "uglygirlsclub\n",
      "fourthwav\n",
      "fashionmajor\n",
      "weresogood\n",
      "samcguigan\n",
      "topmum\n",
      "whenyouseeit\n",
      "'liber\n",
      "neworld\n",
      "torontotrafficisfun\n",
      "kuivaa\n",
      "customcar\n",
      "dartsworldchampionship\n",
      "creso\n",
      "yayaayayayayay\n",
      "'econom\n",
      "whatarethelivingrockpeoplefor\n",
      "bestcoworkersev\n",
      "alwaysinjur\n",
      "sarcasmtweet\n",
      "splitshift\n",
      "fullmcintosh\n",
      "women|\n",
      "misogyny|\n",
      "clashupd\n",
      "youareageniu\n",
      "asgil\n",
      "excitingtim\n",
      "|shock\n",
      "|ummmm\n",
      "|put\n",
      "ongoingproblem\n",
      "childagain\n",
      "pleasedontspeak\n",
      "thankgodforbackup\n",
      "'begin\n",
      "charlesbarkley\n",
      "nypdmutini\n",
      "goodgovern\n",
      "buredin\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 400))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embdict[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 94, 400)           2363200   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 94, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 94, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 94, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 6,210,402\n",
      "Trainable params: 3,847,202\n",
      "Non-trainable params: 2,363,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 400, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False))\n",
    "#model.add(e)\n",
    "#e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)\n",
    "#model.add(e)\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(200, activation='sigmoid'))\n",
    "#model.add(Dropout=0.5)\n",
    "#model.add(Embedding(300, maxSequenceLength))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# compile the model\n",
    "rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "#model.compile(optimizer = rmsprop, loss = 'mean_squared_error', metrics=['mean_squared_error', 'mae'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3222 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      " - 178s - loss: 0.4643 - acc: 0.7682 - val_loss: 0.1758 - val_acc: 0.9100\n",
      "Epoch 2/10\n",
      " - 179s - loss: 0.4512 - acc: 0.7725 - val_loss: 0.4226 - val_acc: 0.7900\n",
      "Epoch 3/10\n",
      " - 179s - loss: 0.4635 - acc: 0.7731 - val_loss: 0.3362 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      " - 177s - loss: 0.4379 - acc: 0.7914 - val_loss: 0.5022 - val_acc: 0.6800\n",
      "Epoch 5/10\n",
      " - 176s - loss: 0.5502 - acc: 0.7207 - val_loss: 0.5259 - val_acc: 0.7500\n",
      "Epoch 6/10\n",
      " - 176s - loss: 0.4435 - acc: 0.7827 - val_loss: 0.2560 - val_acc: 0.9100\n",
      "Epoch 7/10\n",
      " - 176s - loss: 0.4317 - acc: 0.7843 - val_loss: 0.4818 - val_acc: 0.7800\n",
      "Epoch 8/10\n",
      " - 176s - loss: 0.4268 - acc: 0.7948 - val_loss: 0.3345 - val_acc: 0.8500\n",
      "Epoch 9/10\n",
      " - 176s - loss: 0.4222 - acc: 0.8017 - val_loss: 0.5486 - val_acc: 0.7500\n",
      "Epoch 10/10\n",
      " - 176s - loss: 0.4210 - acc: 0.8020 - val_loss: 0.2570 - val_acc: 0.9000\n",
      "Accuracy: 90.000000\n",
      "F1-score: 47.368421\n",
      "Precision: 45.000000\n",
      "Recall: 50.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ors/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ors/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_docs_train, y_train, epochs = 10, verbose=2, validation_data=(padded_docs_test, y_test))\n",
    "predict = np.argmax(model.predict(padded_docs_test), axis=1)\n",
    "answer = np.argmax(y_test, axis=1)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, answer)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, answer, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, answer, average=\"macro\")*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 epochs:\n",
    "- ## test:\n",
    "Accuracy: 87.000000<br>\n",
    "F1-score: 46.524064<br>\n",
    "Precision: 43.500000<br>\n",
    "Recall: 50.000000<br>\n",
    "\n",
    "- ## val:\n",
    "Accuracy: 35.000000<br>\n",
    "F1-score: 25.925926<br>\n",
    "Precision: 17.500000<br>\n",
    "Recall: 50.000000<br>\n",
    "\n",
    "# 20 epochs:\n",
    "- ## test:\n",
    "Accuracy: 90.000000<br>\n",
    "F1-score: 47.368421<br>\n",
    "Precision: 45.000000<br>\n",
    "Recall: 50.000000<br>\n",
    "\n",
    "- ## val:\n",
    "Accuracy: 28.000000<br>\n",
    "F1-score: 21.875000<br>\n",
    "Precision: 14.000000<br>\n",
    "Recall: 50.000000<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 28.000000\n",
      "F1-score: 21.875000\n",
      "Precision: 14.000000\n",
      "Recall: 50.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ors/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ors/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predict = np.argmax(model.predict(padded_docs_val), axis=1)\n",
    "answer = np.argmax(y_val, axis=1)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, answer)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, answer, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, answer, average=\"macro\")*100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
