{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ors/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ors/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import numpy as  np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "path_model = r\"./word2vec_twitter_model.bin\"\n",
    "path_data=r\"/media/ors/ANNA/data_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_test\n",
      "i_train\n",
      "m_test\n",
      "m_train\n",
      "none_test\n",
      "none_train\n",
      "p_test\n",
      "p_train\n",
      "Всего слов: 6515\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) \n",
    "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w)>3]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    text=' '.join(words)\n",
    "    tokens = word_tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    return stemmed\n",
    "    \n",
    "words=set()#множество слов по всем текстам\n",
    "\n",
    "for f in os.listdir(path_data):\n",
    "    print(f)\n",
    "    full_path = path_data+'/'+f\n",
    "    try:\n",
    "        f_text = open(full_path, \"r\").readlines()\n",
    "        for line in f_text:\n",
    "            words_text=clean_text(line)\n",
    "            words.update(words_text)\n",
    "    except Exception as  err:\n",
    "        print(err)\n",
    "        \n",
    "        pass\n",
    "        '''\n",
    "        lines=[]\n",
    "        with open(full_path) as file:\n",
    "            while True:\n",
    "                try:\n",
    "                    line = file.readline()\n",
    "                    lines.append(line)\n",
    "                except:\n",
    "                    pass\n",
    "        for line in lines:\n",
    "            words_text=clean_text(line)\n",
    "            words.update(words_text)\n",
    "        '''\n",
    "    \n",
    "print(\"Всего слов: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ors/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500000\n",
      "iteration 1000000\n",
      "iteration 1500000\n",
      "iteration 2000000\n",
      "iteration 2500000\n",
      "iteration 3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embdict=dict()#словарь эмбеддингов и слов\n",
    "index=0\n",
    "porter = PorterStemmer()\n",
    "\n",
    "with open(path_model,'rb')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode(errors='ignore')\n",
    "            if ch ==' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if len(word) != 0:\n",
    "            tp= np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            word = porter.stem(word.lower())\n",
    "            if word in words:\n",
    "                embdict[str(word)]=tp.tolist()\n",
    "\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "        index+=1\n",
    "        if index%500000==0:\n",
    "            print(\"iteration \"+str(index))\n",
    "print(embdict)\n",
    "print(\"Слов в словаре:\"+str(len(embdict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_test\n",
      "i_train\n",
      "m_test\n",
      "m_train\n",
      "none_test\n",
      "none_train\n",
      "p_test\n",
      "p_train\n",
      "Всего слов: 6515\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    print(f)\n",
    "    full_path = path_data+'/'+f\n",
    "    try:\n",
    "        f_text = open(full_path, \"r\").readlines()\n",
    "        vectors=[]\n",
    "        lines=[]\n",
    "        for line in f_text:\n",
    "            words_text=clean_text(line)\n",
    "            emb_vector=np.zeros(400)\n",
    "            for word in words_text:\n",
    "                try:\n",
    "                    emb_vector+=embdict[word]\n",
    "                except:\n",
    "                    pass\n",
    "            lines.append(line)\n",
    "            vectors.append(emb_vector)\n",
    "        df=pd.DataFrame(vectors)\n",
    "        df.insert(loc=0, column='texts', value=lines)\n",
    "        df.to_csv(f+'.csv',sep=';')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"Всего слов: \"+str(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>дальше код про сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras import utils\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_train_none=r\"none_train.csv\"\n",
    "name_train=r\"i_train.csv\"\n",
    "\n",
    "name_test_none=r\"none_test.csv\"\n",
    "name_test_irony=r\"i_test.csv\"\n",
    "name_test_puns=r\"p_test.csv\"\n",
    "name_test_met=r\"m_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data(path, data_none, flag=False):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        data = pd.read_csv(f, sep=';', header=0, decimal = '.', index_col=0)    \n",
    "    data1 = data.values\n",
    "    data2 = data_none.values\n",
    "    data1 = np.column_stack((np.ones(len(data1)),data1))\n",
    "    data2 = np.column_stack((np.zeros(len(data2)),data2))\n",
    "    data_res=np.vstack((data1,data2)) \n",
    "    if flag:\n",
    "        np.random.shuffle(data_res)\n",
    "    return data_res\n",
    "\n",
    "with open(name_train_none, encoding='utf-8') as f:\n",
    "        df_train_none = pd.read_csv(f, sep=';', header=0, decimal = '.', index_col=0)\n",
    "with open(name_test_none, encoding='utf-8') as f:\n",
    "        df_test_none = pd.read_csv(f, sep=';', header=0, decimal = '.', index_col=0)\n",
    "\n",
    "train = create_data(name_train, df_train_none, True)\n",
    "test_irony = create_data(name_test_irony, df_test_none)\n",
    "test_puns = create_data(name_test_puns, df_test_none)\n",
    "test_met = create_data(name_test_met, df_test_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_floats(texts, categories, vectors):\n",
    "    _texts=[]\n",
    "    _categories=[]\n",
    "    _vectors=[]\n",
    "    for i in range(len(texts)):\n",
    "        if type(texts[i]) is str:\n",
    "            _texts.append(clean_text(texts[i]))\n",
    "            _categories.append(categories[i])\n",
    "            _vectors.append(vectors[i])\n",
    "    return _texts,_categories,_vectors\n",
    "\n",
    "def split_data(d):\n",
    "    df = pd.DataFrame(d)\n",
    "    t = df[1].tolist()\n",
    "    cat = df[0].tolist()\n",
    "    vec= df.drop([0,1], axis=1).values\n",
    "    t,cat,vec=remove_floats(t,cat,vec)\n",
    "    return t,cat,vec\n",
    "\n",
    "texts,categories,vectors=split_data(train)\n",
    "texts_test_irony ,categories_test_irony ,vectors_test_irony =split_data(test_irony)\n",
    "texts_test_puns ,categories_test_puns ,vectors_test_puns =split_data(test_puns)\n",
    "texts_test_met ,categories_test_met ,vectors_test_met =split_data(test_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное количество слов в самом длинном тексте: 94 слов\n",
      "Всего уникальных слов в словаре: 4263\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "descriptions = texts\n",
    "    \n",
    "x_train = texts\n",
    "y_train = categories\n",
    "    \n",
    "x_test_irony = texts_test_irony\n",
    "y_test_irony = categories_test_irony\n",
    "\n",
    "x_test_puns = texts_test_puns\n",
    "y_test_puns = categories_test_puns\n",
    "\n",
    "x_test_met = texts_test_met\n",
    "y_test_met = categories_test_met\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_irony = keras.utils.to_categorical(y_test_irony, num_classes)\n",
    "y_test_puns = keras.utils.to_categorical(y_test_puns, num_classes)\n",
    "y_test_met = keras.utils.to_categorical(y_test_met, num_classes)\n",
    "\n",
    "max_words = 0\n",
    "for desc in descriptions:\n",
    "    try:\n",
    "        words = len(desc)\n",
    "        if words > max_words:\n",
    "            max_words = words\n",
    "    except:\n",
    "        pass\n",
    "print('Максимальное количество слов в самом длинном тексте: {} слов'.format(max_words))\n",
    "\n",
    "maxSequenceLength = max_words\n",
    "\n",
    "t = Tokenizer()\n",
    "    \n",
    "t.fit_on_texts(descriptions)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_docs_train = t.texts_to_sequences(x_train)\n",
    "encoded_docs_test_irony= t.texts_to_sequences(x_test_irony)\n",
    "encoded_docs_test_puns= t.texts_to_sequences(x_test_puns)\n",
    "encoded_docs_test_met= t.texts_to_sequences(x_test_met)\n",
    "padded_docs_train = sequence.pad_sequences(encoded_docs_train, maxlen=maxSequenceLength)\n",
    "padded_docs_test_irony = sequence.pad_sequences(encoded_docs_test_irony, maxlen=maxSequenceLength)\n",
    "padded_docs_test_puns = sequence.pad_sequences(encoded_docs_test_puns, maxlen=maxSequenceLength)\n",
    "padded_docs_test_met = sequence.pad_sequences(encoded_docs_test_met, maxlen=maxSequenceLength)\n",
    "\n",
    "total_unique_words = len(t.word_counts)\n",
    "print('Всего уникальных слов в словаре: {}'.format(total_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 400))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embdict[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "        #print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 94, 400)           1705600   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 94, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 94, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 94, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 5,552,802\n",
      "Trainable params: 3,847,202\n",
      "Non-trainable params: 1,705,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 400, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False))\n",
    "#model.add(e)\n",
    "#e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)\n",
    "#model.add(e)\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(200, activation='sigmoid'))\n",
    "#model.add(Dropout=0.5)\n",
    "#model.add(Embedding(300, maxSequenceLength))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.4, recurrent_dropout=0.2)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# compile the model\n",
    "rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "#model.compile(optimizer = rmsprop, loss = 'mean_squared_error', metrics=['mean_squared_error', 'mae'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      " - 114s - loss: 0.2402 - acc: 0.8920 - val_loss: 1.0037 - val_acc: 0.6750\n",
      "Epoch 2/20\n",
      " - 113s - loss: 0.2586 - acc: 0.8910 - val_loss: 0.5881 - val_acc: 0.7750\n",
      "Epoch 3/20\n",
      " - 114s - loss: 0.2428 - acc: 0.9005 - val_loss: 0.7414 - val_acc: 0.7300\n",
      "Epoch 4/20\n",
      " - 114s - loss: 0.2330 - acc: 0.9090 - val_loss: 0.6509 - val_acc: 0.7475\n",
      "Epoch 5/20\n",
      " - 116s - loss: 0.2471 - acc: 0.8930 - val_loss: 0.5823 - val_acc: 0.7875\n",
      "Epoch 6/20\n",
      " - 114s - loss: 0.2353 - acc: 0.8980 - val_loss: 0.7397 - val_acc: 0.7650\n",
      "Epoch 7/20\n",
      " - 115s - loss: 0.1974 - acc: 0.9240 - val_loss: 0.7327 - val_acc: 0.7225\n",
      "Epoch 8/20\n",
      " - 118s - loss: 0.2175 - acc: 0.9100 - val_loss: 0.6423 - val_acc: 0.7425\n",
      "Epoch 9/20\n",
      " - 114s - loss: 0.2237 - acc: 0.9055 - val_loss: 0.6669 - val_acc: 0.7800\n",
      "Epoch 10/20\n",
      " - 114s - loss: 0.2127 - acc: 0.9155 - val_loss: 0.7957 - val_acc: 0.7125\n",
      "Epoch 11/20\n",
      " - 114s - loss: 0.1970 - acc: 0.9245 - val_loss: 0.7681 - val_acc: 0.7775\n",
      "Epoch 12/20\n",
      " - 115s - loss: 0.1914 - acc: 0.9170 - val_loss: 1.0599 - val_acc: 0.7525\n",
      "Epoch 13/20\n",
      " - 114s - loss: 0.2021 - acc: 0.9150 - val_loss: 0.6046 - val_acc: 0.7600\n",
      "Epoch 14/20\n",
      " - 114s - loss: 0.2140 - acc: 0.9150 - val_loss: 0.7811 - val_acc: 0.7325\n",
      "Epoch 15/20\n",
      " - 113s - loss: 0.1957 - acc: 0.9265 - val_loss: 0.7066 - val_acc: 0.7575\n",
      "Epoch 16/20\n",
      " - 114s - loss: 0.1911 - acc: 0.9285 - val_loss: 0.7566 - val_acc: 0.7700\n",
      "Epoch 17/20\n",
      " - 114s - loss: 0.1880 - acc: 0.9265 - val_loss: 0.9776 - val_acc: 0.7575\n",
      "Epoch 18/20\n",
      " - 113s - loss: 0.1865 - acc: 0.9300 - val_loss: 0.6200 - val_acc: 0.7700\n",
      "Epoch 19/20\n",
      " - 116s - loss: 0.1847 - acc: 0.9315 - val_loss: 0.6886 - val_acc: 0.7475\n",
      "Epoch 20/20\n",
      " - 114s - loss: 0.1811 - acc: 0.9330 - val_loss: 0.7225 - val_acc: 0.7700\n",
      "Precision: 77.000000\n",
      "Recall: 77.904093\n",
      "F1-score: 76.812179\n",
      "Accuracy: 77.000000\n",
      "Precision: 52.750000\n",
      "Recall: 54.930303\n",
      "F1-score: 46.876910\n",
      "Accuracy: 52.750000\n",
      "Precision: 53.250000\n",
      "Recall: 55.692018\n",
      "F1-score: 47.633349\n",
      "Accuracy: 53.250000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_docs_train, y_train, epochs = 20, verbose=2, validation_data=(padded_docs_test_irony, y_test_irony))\n",
    "\n",
    "def predict(padded_docs_test, y_test, name):\n",
    "    predict = np.argmax(model.predict(padded_docs_test), axis=1)\n",
    "    answer = np.argmax(y_test, axis=1)\n",
    "    f=open(name, 'w')\n",
    "    st= 'Precision: %f' % (precision_score(predict, answer, average=\"macro\")*100)\n",
    "    print(st)\n",
    "    f.write(st+'\\n')\n",
    "    st= 'Recall: %f' % (recall_score(predict, answer, average=\"macro\")*100)\n",
    "    print(st)\n",
    "    f.write(st+'\\n')\n",
    "    st= 'F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100)\n",
    "    print(st)\n",
    "    f.write(st+'\\n')\n",
    "    st= 'Accuracy: %f' % (accuracy_score(predict, answer)*100)\n",
    "    print(st)\n",
    "    f.write(st+'\\n')\n",
    "\n",
    "    for p in predict:\n",
    "        f.write(str(p)+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "predict(padded_docs_test_irony, y_test_irony, 'irony_irony_90.txt')\n",
    "predict(padded_docs_test_puns, y_test_puns, 'irony_puns_90.txt')\n",
    "predict(padded_docs_test_met, y_test_met, 'irony_met_90.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60 epochs:\n",
    "Precision: 77.000000<br>\n",
    "Recall: 78.957529<br>\n",
    "F1-score: 76.604618<br>\n",
    "Accuracy: 77.000000<br><br>\n",
    "Precision: 50.250000<br>\n",
    "Recall: 50.679394<br>\n",
    "F1-score: 40.914051<br>\n",
    "Accuracy: 50.250000<br><br>\n",
    "Precision: 53.250000<br>\n",
    "Recall: 57.068675<br>\n",
    "F1-score: 45.950242<br>\n",
    "Accuracy: 53.250000<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
